{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a14094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import correlate, butter, filtfilt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from extract_channels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the tuningTasks files, and \n",
    "# the directory to save granger causality results.\n",
    "data_dir = '/path/to/data/'\n",
    "gc_word_results_dir = data_dir + 'granger_word_trials'\n",
    "results_path = Path(gc_word_results_dir)\n",
    "results_path.mkdir(parents=True, exist_ok=True)\n",
    "fiftyWordDat = sio.loadmat(data_dir+'t12.2022.05.03_fiftyWordSet.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc822745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pval(granger_causality_result):\n",
    "    pval_list = []\n",
    "    for lag in range(1,len(granger_causality_result)+1):\n",
    "        pval = granger_causality_result[lag][0]['ssr_ftest'][1]\n",
    "        pval_list.append(pval)\n",
    "    return pval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0de3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_trial(trial_num, cue, data, caused, causal, maxlag, speaking):\n",
    "    go_start, go_end = data['goTrialEpochs'][trial_num]\n",
    "    \n",
    "    # Define segment length based on speaking condition\n",
    "    expected_length = 40\n",
    "    if speaking:\n",
    "        caused_seg = caused[go_start-15:go_start+25, :]\n",
    "        causal_seg = causal[go_start-15:go_start+25, :]\n",
    "    else:\n",
    "        caused_seg = caused[go_end:go_end+40, :]\n",
    "        causal_seg = causal[go_end:go_end+40, :]\n",
    "    \n",
    "    # Check for insufficient time points\n",
    "    if caused_seg.shape[0] < expected_length or causal_seg.shape[0] < expected_length:\n",
    "        print(f\"Skipping trial {trial_num} due to insufficient time points ({caused_seg.shape[0]} available).\")\n",
    "        return None  # Skip this trial\n",
    "\n",
    "    if isinstance(maxlag, list):\n",
    "        num_lags = len(maxlag)\n",
    "    else:\n",
    "        num_lags = maxlag\n",
    "\n",
    "    cue_res = np.zeros((caused.shape[1], causal.shape[1], num_lags))\n",
    "\n",
    "    for chan0, chan1 in product(range(caused.shape[1]), range(causal.shape[1])):\n",
    "        cmp = np.stack([caused_seg[:, chan0], causal_seg[:, chan1]], axis=1)\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress UserWarnings\n",
    "                gc_res = grangercausalitytests(cmp, maxlag=maxlag, verbose=False)\n",
    "            \n",
    "            result = get_all_pval(gc_res)\n",
    "            if len(result) != num_lags:\n",
    "                print(f\"Shape mismatch in trial {trial_num}: result has {len(result)} lags, expected {num_lags}.\")\n",
    "            \n",
    "            cue_res[chan0, chan1] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Error in trial {trial_num}, channels {chan0}-{chan1}: {e}\")\n",
    "            cue_res[chan0, chan1] = [-1, -1]\n",
    "    \n",
    "    return cue, cue_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72906b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiarray_results(data, caused_chans, causal_chans, maxlag, speaking, data_type='spikePow', n_jobs=1):\n",
    "    caused = extract_channel_data(data[data_type], caused_chans)\n",
    "    causal = extract_channel_data(data[data_type], causal_chans)\n",
    "\n",
    "    processed_results = Parallel(n_jobs=n_jobs)(delayed(process_single_trial)(\n",
    "        trial_num, data['trialCues'][trial_num, 0], data, caused, causal, maxlag, speaking\n",
    "    ) for trial_num in tqdm(range(len(data['trialCues']))))\n",
    "\n",
    "    # Remove None values (skipped trials)\n",
    "    processed_results = [res for res in processed_results if res is not None]\n",
    "\n",
    "    # Maintain order of first appearance (not sorting)\n",
    "    cues = list(dict.fromkeys(cue for cue, _ in processed_results))  # Keeps order\n",
    "\n",
    "    results_dict = {cue: [] for cue in cues}\n",
    "    \n",
    "    for cue, cue_res in processed_results:\n",
    "        results_dict[cue].append(cue_res)\n",
    "    \n",
    "    # Convert list of lists to a numpy object array\n",
    "    results_np = np.array([np.array(results_dict[cue]) for cue in cues], dtype=object)\n",
    "\n",
    "    return cues, results_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f245a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_multiarray_results(cues, results_np, gc_numpy_dir='/home/aidan/data/granger_numpy/', cues_filename='cues.json', results_filename='data.npy'):\n",
    "    # Define output directory\n",
    "    os.makedirs(gc_numpy_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert cues to Python integers (fix JSON serialization issue)\n",
    "    cues_list = [int(cue) for cue in cues]  # Ensures JSON compatibility\n",
    "    \n",
    "    # Save cues as JSON\n",
    "    cues_json_path = os.path.join(gc_numpy_dir, cues_filename)\n",
    "    with open(cues_json_path, 'w') as f:\n",
    "        json.dump(cues_list, f)\n",
    "    print(f\"Saved cues list to {cues_json_path}\")\n",
    "\n",
    "    # Determine the minimum number of trials that exist for all cues\n",
    "    min_trials_per_cue = min([arr.shape[0] for arr in results_np])  # Find the shortest set of trials\n",
    "    num_cues = len(cues)\n",
    "    num_channels = results_np[0].shape[1]  # Should be 64\n",
    "    num_lags = results_np[0].shape[3]      # Should be 12\n",
    "\n",
    "    # Trim all cues to have exactly min_trials_per_cue trials\n",
    "    trimmed_results = np.stack([arr[:min_trials_per_cue] for arr in results_np], axis=0)\n",
    "\n",
    "    # Ensure the final array shape is as expected\n",
    "    assert trimmed_results.shape == (num_cues, min_trials_per_cue, num_channels, num_channels, num_lags), \\\n",
    "        f\"Unexpected shape: {trimmed_results.shape}, expected ({num_cues}, {min_trials_per_cue}, {num_channels}, {num_channels}, {num_lags})\"\n",
    "\n",
    "    # Save the final 5D numpy array\n",
    "    npy_path = os.path.join(gc_numpy_dir, results_filename)\n",
    "    np.save(npy_path, trimmed_results)\n",
    "    print(f\"Saved trimmed numpy array to {npy_path} with shape {trimmed_results.shape}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0bdfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Granger Causality tests for every pair of arrays (Area 44 -> 6v and reverse):\n",
    "\n",
    "# Modify given your resources\n",
    "n_jobs=-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a0d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speaking condition\n",
    "\n",
    "cues, sup44_gc_sup6v_results = generate_multiarray_results(fiftyWordDat, area_6v_superior, area_44_superior, 12, speaking=True, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, sup44_gc_sup6v_results, gc_word_results_dir,\n",
    "                        cues_filename='sup44_gc_sup6v_cues.json', results_filename='sup44_gc_sup6v.npy')\n",
    "\n",
    "cues, sup44_gc_inf6v_results = generate_multiarray_results(fiftyWordDat, area_6v_inferior, area_44_superior, 12, speaking=True, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, sup44_gc_inf6v_results, gc_word_results_dir,\n",
    "                        cues_filename='sup44_gc_inf6v_cues.json', results_filename='sup44_gc_inf6v.npy')\n",
    "\n",
    "cues, inf44_gc_sup6v_results = generate_multiarray_results(fiftyWordDat, area_6v_superior, area_44_inferior, 12, speaking=True, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, inf44_gc_sup6v_results, gc_word_results_dir,\n",
    "                        cues_filename='inf44_gc_sup6v_cues.json', results_filename='inf44_gc_sup6v.npy')\n",
    "\n",
    "cues, inf44_gc_inf6v_results = generate_multiarray_results(fiftyWordDat, area_6v_inferior, area_44_inferior, 12, speaking=True, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, inf44_gc_inf6v_results, gc_word_results_dir,\n",
    "                        cues_filename='inf44_gc_inf6v_cues.json', results_filename='inf44_gc_inf6v.npy')\n",
    "\n",
    "# Swapped brain areas\n",
    "cues, sup6v_gc_sup44_results = generate_multiarray_results(fiftyWordDat, area_44_superior, area_6v_superior, 12, speaking=True, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, sup6v_gc_sup44_results, gc_word_results_dir,\n",
    "                        cues_filename='sup6v_gc_sup44_cues.json', results_filename='sup6v_gc_sup44.npy')\n",
    "\n",
    "cues, sup6v_gc_inf44_results = generate_multiarray_results(fiftyWordDat, area_44_inferior, area_6v_superior, 12, speaking=True, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, sup6v_gc_inf44_results, gc_word_results_dir,\n",
    "                        cues_filename='sup6v_gc_inf44_cues.json', results_filename='sup6v_gc_inf44.npy')\n",
    "\n",
    "cues, inf6v_gc_sup44_results = generate_multiarray_results(fiftyWordDat, area_44_superior, area_6v_inferior, 12, speaking=True, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, inf6v_gc_sup44_results, gc_word_results_dir,\n",
    "                        cues_filename='inf6v_gc_sup44_cues.json', results_filename='inf6v_gc_sup44.npy')\n",
    "\n",
    "cues, inf6v_gc_inf44_results = generate_multiarray_results(fiftyWordDat, area_44_inferior, area_6v_inferior, 12, speaking=True, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, inf6v_gc_inf44_results, gc_word_results_dir,\n",
    "                        cues_filename='inf6v_gc_inf44_cues.json', results_filename='inf6v_gc_inf44.npy')\n",
    "\n",
    "# Non-speaking condition (control)\n",
    "cues, ctrl_sup44_gc_sup6v_results = generate_multiarray_results(fiftyWordDat, area_6v_superior, area_44_superior, 12, speaking=False, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, ctrl_sup44_gc_sup6v_results, gc_word_results_dir,\n",
    "                        cues_filename='ctrl_sup44_gc_sup6v_cues.json', results_filename='ctrl_sup44_gc_sup6v.npy')\n",
    "\n",
    "cues, ctrl_sup44_gc_inf6v_results = generate_multiarray_results(fiftyWordDat, area_6v_inferior, area_44_superior, 12, speaking=False, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, ctrl_sup44_gc_inf6v_results, gc_word_results_dir,\n",
    "                        cues_filename='ctrl_sup44_gc_inf6v_cues.json', results_filename='ctrl_sup44_gc_inf6v.npy')\n",
    "\n",
    "cues, ctrl_inf44_gc_sup6v_results = generate_multiarray_results(fiftyWordDat, area_6v_superior, area_44_inferior, 12, speaking=False, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, ctrl_inf44_gc_sup6v_results, gc_word_results_dir,\n",
    "                        cues_filename='ctrl_inf44_gc_sup6v_cues.json', results_filename='ctrl_inf44_gc_sup6v.npy')\n",
    "\n",
    "cues, ctrl_inf44_gc_inf6v_results = generate_multiarray_results(fiftyWordDat, area_6v_inferior, area_44_inferior, 12, speaking=False, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, ctrl_inf44_gc_inf6v_results, gc_word_results_dir,\n",
    "                        cues_filename='ctrl_inf44_gc_inf6v_cues.json', results_filename='ctrl_inf44_gc_inf6v.npy')\n",
    "\n",
    "# Swapped brain areas\n",
    "cues, ctrl_sup6v_gc_sup44_results = generate_multiarray_results(fiftyWordDat, area_44_superior, area_6v_superior, 12, speaking=False, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, ctrl_sup6v_gc_sup44_results, gc_word_results_dir,\n",
    "                        cues_filename='ctrl_sup6v_gc_sup44_cues.json', results_filename='ctrl_sup6v_gc_sup44.npy')\n",
    "\n",
    "cues, ctrl_sup6v_gc_inf44_results = generate_multiarray_results(fiftyWordDat, area_44_inferior, area_6v_superior, 12, speaking=False, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, ctrl_sup6v_gc_inf44_results, gc_word_results_dir,\n",
    "                        cues_filename='ctrl_sup6v_gc_inf44_cues.json', results_filename='ctrl_sup6v_gc_inf44.npy')\n",
    "\n",
    "cues, ctrl_inf6v_gc_sup44_results = generate_multiarray_results(fiftyWordDat, area_44_superior, area_6v_inferior, 12, speaking=False, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, ctrl_inf6v_gc_sup44_results, gc_word_results_dir,\n",
    "                        cues_filename='ctrl_inf6v_gc_sup44_cues.json', results_filename='ctrl_inf6v_gc_sup44.npy')\n",
    "\n",
    "cues, ctrl_inf6v_gc_inf44_results = generate_multiarray_results(fiftyWordDat, area_44_inferior, area_6v_inferior, 12, speaking=False, n_jobs=n_jobs)\n",
    "save_multiarray_results(cues, ctrl_inf6v_gc_inf44_results, gc_word_results_dir,\n",
    "                        cues_filename='ctrl_inf6v_gc_inf44_cues.json', results_filename='ctrl_inf6v_gc_inf44.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
